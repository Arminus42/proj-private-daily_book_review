import feedparser
import requests
from bs4 import BeautifulSoup
import json
import os
import time
from datetime import datetime, timedelta

# --- ì„¤ì • ---

# 1. í¬ë¡¤ë§ ëŒ€ìƒ ì‹ ë¬¸ì‚¬ RSS ë° ë³¸ë¬¸ ì¶”ì¶œ ì„ íƒì(Selector) ì •ì˜
# (ìˆ˜ì§‘ ì„±ê³µë¥ ì´ í™•ì¸ëœ 5ê°œ ë©”ì´ì € ë§¤ì²´)
NEWSPAPERS = [
    # 1. í•œê²¨ë ˆ (ë¬¸í™” ì¼ë°˜: ì„œí‰ í€„ë¦¬í‹° ë†’ìŒ)
    {
        "name": "í•œê²¨ë ˆ", "id": "hani",
        "url": "https://www.hani.co.kr/rss/culture/",
        "selector": ".article-text, .text, #a-left-scroll-in"
    },
    # 2. ë§¤ì¼ê²½ì œ (ë¬¸í™”/ì—°ì˜ˆ: ë°ì´í„° ìˆ˜ì‹  ì•ˆì •ì )
    {
        "name": "ë§¤ì¼ê²½ì œ", "id": "mk",
        "url": "https://www.mk.co.kr/rss/30000023/",
        "selector": ".news_cnt_detail_wrap, .view_txt, .art_txt"
    },
    # 3. í•œêµ­ê²½ì œ (ìƒí™œ/ë¬¸í™”: ì„œí‰ ë° ì¹¼ëŸ¼ í’ë¶€)
    {
        "name": "í•œêµ­ê²½ì œ", "id": "hankyung",
        "url": "https://www.hankyung.com/feed/life",
        "selector": "#articletxt, .article-body"
    },
    # 4. ì˜¤ë§ˆì´ë‰´ìŠ¤ (ì±… ì „ë¬¸: ê°€ì¥ í™•ì‹¤í•œ ì„œí‰ ì†ŒìŠ¤)
    {
        "name": "ì˜¤ë§ˆì´ë‰´ìŠ¤", "id": "ohmynews",
        "url": "http://rss.ohmynews.com/rss/book.xml",
        "selector": ".article_view, .at_contents"
    },
    # 5. ë™ì•„ì¼ë³´ (ë¬¸í™”: RSS ì‚´ì•„ìˆìŒ)
    {
        "name": "ë™ì•„ì¼ë³´", "id": "donga",
        "url": "https://rss.donga.com/culture.xml",
        "selector": ".article_txt, .article_view, #article_txt"
    }
]

# 2. í•„í„°ë§ í‚¤ì›Œë“œ
REQUIRED_KEYWORDS = ["ì±…", "ì„œí‰", "ë„ì„œ", "ì¶œíŒ", "ì‹ ê°„", "ì‘ê°€", "ì €ì", "ì†Œì„¤", "ì—ì„¸ì´", "ë¬¸í•™", "ì¸ë¬¸", "ë…ì„œ", "ë² ìŠ¤íŠ¸ì…€ëŸ¬", "ì½ê¸°", "ì„œì "]
EXCLUDE_KEYWORDS = ["ì˜í™”", "ë“œë¼ë§ˆ", "ë°©ì†¡", "ê³µì—°", "ì „ì‹œ", "í™”ë³´", "í¬í† ", "ì—¬í–‰", "ë‚ ì”¨", "ë¶€ê³ ", "ë§›ì§‘", "ê°œë´‰", "ì‹œì²­ë¥ ", "ì˜ˆëŠ¥", "ë³„ì„¸", "ì¸ì‚¬", "ë™ì •"]

def clean_text(text):
    """ë¶ˆí•„ìš”í•œ ê³µë°± ë° ë…¸ì´ì¦ˆ ë¬¸êµ¬(TTS, ê´‘ê³  ë“±) ì œê±°"""
    if not text:
        return ""
    
    # ë…¸ì´ì¦ˆ í•„í„°ë§ í‚¤ì›Œë“œ (ì´ ë¬¸êµ¬ê°€ í¬í•¨ëœ ì¤„ì€ ì‚­ì œ)
    NOISE_PATTERNS = [
        "ê¸°ì‚¬ë¥¼ ì½ì–´ë“œë¦½ë‹ˆë‹¤",
        "Your browser does not support",
        "audio element",
        "audio",
        "0:00",
        "ì‚¬ì§„ í™•ëŒ€",
        "ê´‘ê³ ",
        "ë°°ë„ˆ",
        "ë‹«ê¸°",
        "ê¸°ì",  # ë‹¨ìˆœíˆ 'OOO ê¸°ì'ë§Œ ìˆëŠ” ì¤„ ì œê±°ìš©
        "ì´ë©”ì¼",
        "êµ¬ë…"
    ]
    
    cleaned_lines = []
    for line in text.splitlines():
        line = line.strip()
        if not line: continue
        
        # 1. ë…¸ì´ì¦ˆ íŒ¨í„´ ê²€ì‚¬
        is_noise = False
        for pattern in NOISE_PATTERNS:
            if pattern in line:
                is_noise = True
                break
        if is_noise: continue
        
        # 2. ë„ˆë¬´ ì§§ì€, ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´ë§Œ ìˆëŠ” ì¤„ ì œê±° (ë‹¨, í•µì‹¬ ë‹¨ì–´ëŠ” ì œì™¸)
        if len(line) < 2 and line not in ["ì±…", "ì‚¶", "ì‹œ", "ê¿ˆ", "ë"]:
             continue

        cleaned_lines.append(line)
        
    return "\n\n".join(cleaned_lines)

def fetch_rss_feed(url):
    """requestsë¥¼ ì‚¬ìš©í•˜ì—¬ RSS XML ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¨ í›„ feedparserë¡œ íŒŒì‹± (ì°¨ë‹¨ ìš°íšŒìš©)"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        'Referer': 'https://www.google.com/'
    }
    try:
        res = requests.get(url, headers=headers, timeout=15)
        res.encoding = res.apparent_encoding # í•œê¸€ ê¹¨ì§ ë°©ì§€
        if res.status_code != 200:
            return None
        return feedparser.parse(res.content)
    except Exception as e:
        print(f"   âŒ RSS ìš”ì²­ ì‹¤íŒ¨: {e}")
        return None

def get_article_content(url, selector):
    """URLì—ì„œ HTMLì„ ê°€ì ¸ì™€ selectorì— í•´ë‹¹í•˜ëŠ” ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê°•ë ¥í•œ í•„í„°ë§ ì ìš©)"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        res = requests.get(url, headers=headers, timeout=10)
        res.encoding = res.apparent_encoding 
        
        if res.status_code != 200:
            return None

        soup = BeautifulSoup(res.text, 'html.parser')

        # 1. ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±° (DOM ë‹¨ê³„ì—ì„œ ì‚­ì œ)
        trash_selectors = [
            # ê¸°ë³¸ ë¶ˆí•„ìš” íƒœê·¸
            'script', 'style', 'iframe', 'header', 'footer', 'nav', 
            '.ad', '.ads', '.ad_box', '#sidebar', 
            '.reporter_area', '.copyright', '.related_news', '.rel_news',
            
            # TTS ë° ì˜¤ë””ì˜¤ ê´€ë ¨ (ë…¸ì´ì¦ˆ ì›í‰)
            '.tts_box', '.audiop', '.audio-player', '.btn_tts', '.vod_player',
            
            # ì´ë¯¸ì§€ ìº¡ì…˜ ë° í™•ëŒ€ ë²„íŠ¼
            'figcaption', '.img_desc', '.desc_txt', '.img_caption', '.caption', 
            '.photo_zoom', '.zoom_btn', '.btn_photo_zoom', '.image-area'
        ]
        
        for selector_str in trash_selectors:
            for trash in soup.select(selector_str):
                trash.decompose()

        # 2. ì§€ì •ëœ ì„ íƒìë¡œ ë³¸ë¬¸ ì°¾ê¸°
        content_area = soup.select_one(selector)
        
        # 3. Fallback: ì¼ë°˜ì ì¸ ë³¸ë¬¸ íƒœê·¸ ê²€ìƒ‰
        if not content_area:
            content_area = soup.select_one('article, #article_body, .news_view, div[itemprop="articleBody"]')

        if content_area:
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ í›„ 2ì°¨ í•„í„°ë§(clean_text) ìˆ˜í–‰
            return clean_text(content_area.get_text(separator='\n'))
        else:
            return None

    except Exception as e:
        # print(f"      âš ï¸ ë³¸ë¬¸ ì—ëŸ¬: {e}")
        return None

def crawl_and_extract():
    today = datetime.now().date()
    limit_date = today - timedelta(days=5) # ìµœê·¼ 5ì¼ì¹˜
    
    print(f"ğŸ“… [í¬ë¡¤ë§ ì‹œì‘] ({limit_date} ~ {today})")
    print("-" * 50)
    
    all_data = {}

    for paper in NEWSPAPERS:
        print(f"\nğŸ“° [{paper['name']}] RSS í™•ì¸ ì¤‘...")
        
        feed = fetch_rss_feed(paper['url'])
        
        if not feed or not feed.entries:
            print("   âš ï¸ RSS í”¼ë“œë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (ì°¨ë‹¨ ë˜ëŠ” ë¹„ì–´ìˆìŒ).")
            continue
            
        print(f"   âœ… RSS ìˆ˜ì‹  ì™„ë£Œ: {len(feed.entries)}ê°œ í•­ëª©")
        
        articles = []
        count = 0
        
        for entry in feed.entries:
            if count >= 3: # ì‹ ë¬¸ì‚¬ë³„ ìµœëŒ€ 3ê°œ (í…ŒìŠ¤íŠ¸ìš©)
                break

            # ë‚ ì§œ íŒŒì‹±
            pub_struct = entry.get('published_parsed')
            if pub_struct:
                pub_date = datetime(*pub_struct[:6]).date()
            else:
                pub_date = today

            if pub_date < limit_date:
                continue
            
            title = entry.get('title', '')
            link = entry.get('link', '')
            
            # ì œëª© í•„í„°ë§
            if any(kw in title for kw in EXCLUDE_KEYWORDS):
                continue
            if not any(kw in title for kw in REQUIRED_KEYWORDS):
                continue
            
            print(f"   ğŸ” [ìˆ˜ì§‘] {title[:20]}... ({pub_date})")
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            full_text = get_article_content(link, paper['selector'])
            
            if not full_text or len(full_text) < 200:
                print("      ã„´ ğŸš« ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ (ë‚´ìš© ì§§ìŒ)")
                continue
            
            # ì´ë¯¸ì§€ ì¶”ì¶œ
            img_src = ""
            if 'media_content' in entry:
                img_src = entry.media_content[0]['url']
            elif 'enclosure' in entry:
                 img_src = entry.enclosure.get('href', '')

            articles.append({
                "source": paper['name'],
                "title": title,
                "link": link,
                "date": pub_date.strftime("%Y-%m-%d"),
                "image": img_src,
                "content": full_text[:200] + "...", # ë¯¸ë¦¬ë³´ê¸°ìš©
                "full_text": full_text              # ì „ì²´ ë³¸ë¬¸
            })
            
            count += 1
            time.sleep(0.5) # ê³¼ë„í•œ ìš”ì²­ ë°©ì§€
        
        if articles:
            all_data[paper['id']] = articles
            print(f"   ğŸ’¾ {len(articles)}ê°œ ê¸°ì‚¬ ì €ì¥ë¨.")
        else:
            print("   ğŸ’¨ ì¡°ê±´ì— ë§ëŠ” ì„œí‰ ê¸°ì‚¬ ì—†ìŒ.")

        time.sleep(1)

    # ê²°ê³¼ ì €ì¥
    public_dir = os.path.join(os.path.dirname(__file__), '../public')
    os.makedirs(public_dir, exist_ok=True)
    output_path = os.path.join(public_dir, 'data.json')
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(all_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸš€ [ì™„ë£Œ] ê²°ê³¼ ì €ì¥ë¨: {output_path}")

if __name__ == "__main__":
    crawl_and_extract()